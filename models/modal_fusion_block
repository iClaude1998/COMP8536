'''
Author: Yunxiang Liu u7191378@anu.edu.au
Date: 2022-10-10 22:29:55
LastEditors: Yunxiang Liu u7191378@anu.edu.au
LastEditTime: 2022-10-11 01:04:20
FilePath: \HoiTransformer\models\modal_fusion_block
Description: Grouping_block
'''
import torch
import numpy as np
from torch import nn 


class word_fusion_block(nn.Module):
    
    def __init__(self, queries_dim, emb_dim, world_embedding_path, gumbel=True, tau=1., num_heads=8, device='cuda', drop_out=.1):
        super().__init__()
        self.glove_world_embedding = torch.from_numpy(np.load(world_embedding_path)).to(device)
        self.atten(queries_dim, emb_dim, self.glove_world_embedding, gumbel, tau, num_heads, drop_out)
        self.connection = SubLayerConnection(emb_dim)
        
    def forward(self, queries):
        return self.connection(queries, lambda queries: self.atten(queries))
    
    

class word_attention(nn.Module):
    
    def __init__(self, queries_dim, emb_dim, word_embedding, gumbel=True, tau=1., num_heads=8, drop_out=.1):
        super().__init__()
        assert emb_dim % num_heads == 0, 'The embedded dimension should be divisible by number of heads'
        self.emb_dim = emb_dim
        self.glove_word_embedding = word_embedding
        self.num_words, self.word_dim = self.glove_world_embedding.size()
        self.num_heads = num_heads
        self.dim_head = emb_dim // self.num_heads
        self.scale = self.dim_head ** -0.5
        self.query_dim = queries_dim
        self.to_q = nn.Linear(self.query_dim, emb_dim, bias=False)
        self.to_k = nn.Linear(self.word_dim, emb_dim, bias=False)
        self.to_v = nn.Linear(self.word_dim, emb_dim, bias=False)
        self.to_out = nn.Sequential(nn.Linear(emb_dim, self.query_dim), nn.Dropout(drop_out))
        self.gumbel = gumbel
        self.tau = tau
    
    def forward(self, queries):
        num_queries, _ = queries.size()
        q = self.to_q(queries)
        k = self.to_k(self.glove_word_embedding)
        v = self.to_v(self.glove_word_embedding)
        
        q = q.reshape(num_queries, self.num_heads, self.dim_head).transpose(0, 1)
        k = k.reshape(self.num_words, self.num_heads, self.dim_head).transpose(0, 1)
        v = v.reshape(self.num_words, self.num_heads, self.dim_head).transpose(0, 1)
        
        dots = torch.matmul(q, k.transpose(1, 2)) * self.scale
        if self.gumbel:
            attn = gumbel_softmax(dots, hard=True)
        else:
            attn = dots.softmax(-1)
        out = torch.matmul(attn, v).tranpose(0, 1).reshape(num_queries, self.emb_dim)
        return self.to_out(out)
            
        
class SubLayerConnection(nn.Module):

    def __init__(self, size):
        super(SubLayerConnection, self).__init__()
        self.norm = nn.LayerNorm(size)


    def forward(self, x, sublayer):
        return x + sublayer(self.norm(x))

    
def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False) -> torch.Tensor:
    # _gumbels = (-torch.empty_like(
    #     logits,
    #     memory_format=torch.legacy_contiguous_format).exponential_().log()
    #             )  # ~Gumbel(0,1)
    # more stable https://github.com/pytorch/pytorch/issues/41663
    gumbel_dist = torch.distributions.gumbel.Gumbel(
        torch.tensor(0., device=logits.device, dtype=logits.dtype),
        torch.tensor(1., device=logits.device, dtype=logits.dtype))
    gumbels = gumbel_dist.sample(logits.shape)

    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)
    y_soft = gumbels.softmax(-1)

    if hard:
        # Straight through.
        index = y_soft.max(-1, keepdim=True)[1]
        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(-1, index, 1.0)
        ret = y_hard - y_soft.detach() + y_soft
    else:
        # Reparametrization trick.
        ret = y_soft
    return ret